# Credit-Card-Fraud-Detection
The problem statement for this project is to predict fraudulent credit card transactions with the help of various machine learning models. The major aim is to deal with the classification of a data point into Fraud or Not Fraud with the help of Classification Models such as Logistic Regression, K-Nearest Neighbors, Decision Trees, Random Forests, and XGBoost. 
On initial analysis of the data, we can notice that almost all of the data points are Gaussian i.e. they have undergone principal component analysis and the columns that can be seen in the data set are principal components of each of the data points. This is mainly done so that the data remains confidential and the details of card transactions are not exposed. We can also notice that data is highly imbalanced i.e. there are a total of 2,84,807 transactions, out of which only 492 are fraudulent. This can be treated with the help of Random Oversampling, SMOTE, or ADASYN.
Since the data given to us is classified but is transformed we can emphasize more Logistic Regression, K-Nearest Neighbors, and XGBoost. Our approach towards this project would first initially begin with importing the necessary packages and understanding the data. Check for missing values if any and also conduct exploratory data analysis of each of the transformed variables. One such important EDA step would be to check the skewness of the variable by plotting histograms and to treat it to ensure that the distribution is more Gaussian than before.
We then proceed to build models without balancing the data. One thing we make sure of here is to use the Stratified K-Fold so that there is a constant ratio present in the classes throughout the different train and validation(test) sets. We then proceed to perform hyperparameter tuning for each of the algorithms to get the best set of hyperparameters using which we build the final model and test it on our test data. Here we do not use evaluation metrics such as accuracy, but we use Area Under the ROC Curve to check how well our model is performing. We also check for the optimal threshold for which the Area under the ROC Curve is the highest and accordingly compute other metrics such as precision and recall.
Once we select the best model from the above algorithms, we proceed to build the model after balancing the classes. Here we mainly use three types of balancing techniques i.e. Random Oversampling, SMOTE, and ADASYN. For each of these balancing techniques, we follow the complete procedure of building different models and verifying and evaluating them individually. Here we can use K-Fold validation since our data is balanced. We finally select the best model based on the evaluation metrics mentioned above. Finally, we check for the important features of our model though they are transformed as it will help us understand the dataset is not PCA-transformed.
